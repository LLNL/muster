<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<title>Muster: doc/mainpage Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link href="doxygen.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<!-- Generated by Doxygen 1.7.1 -->
<div class="navigation" id="top">
  <div class="tabs">
    <ul class="tablist">
      <li><a href="main.html"><span>Main&nbsp;Page</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li class="current"><a href="files.html"><span>Files</span></a></li>
    </ul>
  </div>
  <div class="tabs2">
    <ul class="tablist">
      <li><a href="files.html"><span>File&nbsp;List</span></a></li>
      <li><a href="globals.html"><span>File&nbsp;Members</span></a></li>
    </ul>
  </div>
<div class="header">
  <div class="headertitle">
<h1>doc/mainpage</h1>  </div>
</div>
<div class="contents">
<a href="mainpage.html">Go to the documentation of this file.</a><div class="fragment"><pre class="fragment"><a name="l00001"></a>00001 <span class="comment">// -*- c++ -*-</span>
<a name="l00002"></a>00002 <span class="comment"></span>
<a name="l00003"></a>00003 <span class="comment">/*! </span>
<a name="l00004"></a>00004 <span class="comment">  @mainpage</span>
<a name="l00005"></a>00005 <span class="comment"></span>
<a name="l00006"></a>00006 <span class="comment">  \image html kmedoids.png</span>
<a name="l00007"></a>00007 <span class="comment">    </span>
<a name="l00008"></a>00008 <span class="comment">  &lt;h2&gt;Introduction&lt;/h2&gt;</span>
<a name="l00009"></a>00009 <span class="comment"></span>
<a name="l00010"></a>00010 <span class="comment">  The Muster library provides implementations of sequential and parallel K-Medoids </span>
<a name="l00011"></a>00011 <span class="comment">  clustering algorithms.  It is intended as a general framework for parallel </span>
<a name="l00012"></a>00012 <span class="comment">  cluster analysis, particularly for performance data analysis on systems with </span>
<a name="l00013"></a>00013 <span class="comment">  very large numbers of processes.</span>
<a name="l00014"></a>00014 <span class="comment"></span>
<a name="l00015"></a>00015 <span class="comment">  The parallel implementations in the Muster are designed to perform well even</span>
<a name="l00016"></a>00016 <span class="comment">  in environments where the data to be clustered is entirely distributed.  For</span>
<a name="l00017"></a>00017 <span class="comment">  example, many performance tools need to analyze one data element from each</span>
<a name="l00018"></a>00018 <span class="comment">  process in a system.  To analyze this data efficiently, clustering algorithms </span>
<a name="l00019"></a>00019 <span class="comment">  that move as little data as possible are required.  In Muster, we exploit </span>
<a name="l00020"></a>00020 <span class="comment">  sampled clustering algorithms to realize this efficiency.</span>
<a name="l00021"></a>00021 <span class="comment">  </span>
<a name="l00022"></a>00022 <span class="comment">  The parallel algorithms in Muster are implemented using the Message Passing </span>
<a name="l00023"></a>00023 <span class="comment">  Interface (MPI), making them suitable for use on many of the world&#39;s largest </span>
<a name="l00024"></a>00024 <span class="comment">  supercomputers.  They should, however, also run efficiently on your laptop.</span>
<a name="l00025"></a>00025 <span class="comment">  </span>
<a name="l00026"></a>00026 <span class="comment">  &lt;h2&gt;Getting Started&lt;/h2&gt;</span>
<a name="l00027"></a>00027 <span class="comment">  </span>
<a name="l00028"></a>00028 <span class="comment">  &lt;h3&gt;Partitions&lt;/h3&gt;</span>
<a name="l00029"></a>00029 <span class="comment">  The algorithms in Muster are &lt;i&gt;partitioning algorithms&lt;/i&gt;, that is, they divide</span>
<a name="l00030"></a>00030 <span class="comment">  a data set up into a set of groups, or &lt;i&gt;clusters&lt;/i&gt; of data objects.  Together, </span>
<a name="l00031"></a>00031 <span class="comment">  these groups are called a &lt;i&gt;clustering&lt;/i&gt; or a &lt;i&gt;partitioning&lt;/i&gt; of the data.</span>
<a name="l00032"></a>00032 <span class="comment"></span>
<a name="l00033"></a>00033 <span class="comment">  There are two classes that represent clusterings.  These are as follows:</span>
<a name="l00034"></a>00034 <span class="comment">  &lt;dl&gt;</span>
<a name="l00035"></a>00035 <span class="comment">  &lt;dt&gt;\link cluster::partition partition\endlink&lt;/dt&gt;</span>
<a name="l00036"></a>00036 <span class="comment">  &lt;dd&gt;This class represents a partitioning of a data set.  It stores the clusters in the </span>
<a name="l00037"></a>00037 <span class="comment">  data set along with &lt;i&gt;representatives&lt;/i&gt; from each of the clusters.  It also stores,</span>
<a name="l00038"></a>00038 <span class="comment">  for every object in the data set, the cluster to which that object has been assigned.</span>
<a name="l00039"></a>00039 <span class="comment"></span>
<a name="l00040"></a>00040 <span class="comment">  A \link cluster::partition partition\endlink is entirely local to the process that owns it.  It exists </span>
<a name="l00041"></a>00041 <span class="comment">  in one memory space and contains data about &lt;i&gt;all&lt;/i&gt; objects in the data set it describes.</span>
<a name="l00042"></a>00042 <span class="comment">  &lt;/dd&gt;</span>
<a name="l00043"></a>00043 <span class="comment"></span>
<a name="l00044"></a>00044 <span class="comment">  &lt;dt&gt;\link cluster::par_partition par_partition\endlink&lt;/dt&gt;</span>
<a name="l00045"></a>00045 <span class="comment">  &lt;dd&gt;This class is similar to \link cluster::partition partition\endlink, but it is a distributed </span>
<a name="l00046"></a>00046 <span class="comment">  data structure. A \link cluster::par_partition par_partition\endlink object represents the </span>
<a name="l00047"></a>00047 <span class="comment">  results of parallel clustering algorithm.</span>
<a name="l00048"></a>00048 <span class="comment">  Instead of storing the cluster assignments of &lt;i&gt;all&lt;/i&gt; objects in the data set, each </span>
<a name="l00049"></a>00049 <span class="comment">  par_partition object stores only the cluster membership for objects that are local to the</span>
<a name="l00050"></a>00050 <span class="comment">  calling process.</span>
<a name="l00051"></a>00051 <span class="comment">  &lt;/dd&gt;</span>
<a name="l00052"></a>00052 <span class="comment">  &lt;/dl&gt;</span>
<a name="l00053"></a>00053 <span class="comment"></span>
<a name="l00054"></a>00054 <span class="comment">  Note that par_partition does not inherit from partition, because the classes are structurally</span>
<a name="l00055"></a>00055 <span class="comment">  different.  One is a local, centralized data structure, and its state represents all data in the </span>
<a name="l00056"></a>00056 <span class="comment">  set, while the other is a distributed structure, and represents only a part of the full data set.</span>
<a name="l00057"></a>00057 <span class="comment">  </span>
<a name="l00058"></a>00058 <span class="comment">  If you need to, you can convert a par_partition to a partition with the </span>
<a name="l00059"></a>00059 <span class="comment">  \link cluster::par_partition::gather() par_partition::gather()\endlink method, but the two </span>
<a name="l00060"></a>00060 <span class="comment">  classes are not interchangeable.</span>
<a name="l00061"></a>00061 <span class="comment"></span>
<a name="l00062"></a>00062 <span class="comment">  &lt;h3&gt;Clustering Algorithms&lt;/h3&gt;</span>
<a name="l00063"></a>00063 <span class="comment">  Classes for clustering algorithms derive from either \link cluster::partition partition\endlink </span>
<a name="l00064"></a>00064 <span class="comment">  or \link cluster::par_partition par_partition\endlink.  The algorithms themselves are methods on these derived </span>
<a name="l00065"></a>00065 <span class="comment">  classes, and they store their output in the class.  This allows all (or at least most of)</span>
<a name="l00066"></a>00066 <span class="comment">  the state for the algorithms and their output to be encapsulated in a single class.  </span>
<a name="l00067"></a>00067 <span class="comment"></span>
<a name="l00068"></a>00068 <span class="comment">  Algorithms themselves are template functions on the derived classes.  You can thus call them</span>
<a name="l00069"></a>00069 <span class="comment">  on any type of object with any number of distance metrics.  Because they are template</span>
<a name="l00070"></a>00070 <span class="comment">  functions, you don&#39;t need to explicitly indicate the types of the things you pass to the </span>
<a name="l00071"></a>00071 <span class="comment">  clustering algorithms; the types are inferred from the algorithms&#39; parameters.</span>
<a name="l00072"></a>00072 <span class="comment">  </span>
<a name="l00073"></a>00073 <span class="comment">  There are two classes you should be concerned with:</span>
<a name="l00074"></a>00074 <span class="comment"></span>
<a name="l00075"></a>00075 <span class="comment">  &lt;dl&gt;</span>
<a name="l00076"></a>00076 <span class="comment">  &lt;dt&gt;\link cluster::kmedoids kmedoids\endlink&lt;/dt&gt;</span>
<a name="l00077"></a>00077 <span class="comment">  &lt;dd&gt;This class inherits from \link cluster::partition partition\endlink and  contains implementations </span>
<a name="l00078"></a>00078 <span class="comment">  of the PAM and CLARA sequential clustering algorithms (Kaufman and Rousseeuw, 1990).</span>
<a name="l00079"></a>00079 <span class="comment">  PAM is an \f$O(n^2)\f$, optimal K-medoids algorithm, and CLARA is an \f$O(n)\f$ implementation</span>
<a name="l00080"></a>00080 <span class="comment">  that executes multiple trials of PAM.  These algorithms are implemented in the </span>
<a name="l00081"></a>00081 <span class="comment">  \link cluster::kmedoids::pam() pam()\endlink and \link cluster::kmedoids::clara() clara()\endlink methods.  </span>
<a name="l00082"></a>00082 <span class="comment"></span>
<a name="l00083"></a>00083 <span class="comment">  PAM and CLARA both require that the caller supply \f$k\f$, the number of clusters, as a </span>
<a name="l00084"></a>00084 <span class="comment">  parameter to the algorithm. We have adopted a technique used by the </span>
<a name="l00085"></a>00085 <span class="comment">  X-Means (Pelleg and Moore, 2000) algorithm to choose an &quot;ideal&quot; \f$k\f$ from many clustering</span>
<a name="l00086"></a>00086 <span class="comment">  trials using the \link bic.h Bayesian Information Criterion (BIC)\endlink.  Instead of supplying a </span>
<a name="l00087"></a>00087 <span class="comment">  specific \f$k\f$, the caller supplies a range of values for \f$k\f$, and the algorithms </span>
<a name="l00088"></a>00088 <span class="comment">  use the BIC to select the best fit from the range.</span>
<a name="l00089"></a>00089 <span class="comment"></span>
<a name="l00090"></a>00090 <span class="comment">  The BIC variants are implemented in </span>
<a name="l00091"></a>00091 <span class="comment">  \link cluster::kmedoids::xpam() xpam()\endlink and \link cluster::kmedoids::xclara() xclara()\endlink.  </span>
<a name="l00092"></a>00092 <span class="comment">  They will be slower than the fixed-k versions, as they can require many iterations of PAM or CLARA be tested</span>
<a name="l00093"></a>00093 <span class="comment">  to find the best \f$k\f$.</span>
<a name="l00094"></a>00094 <span class="comment">  &lt;/dd&gt;</span>
<a name="l00095"></a>00095 <span class="comment"></span>
<a name="l00096"></a>00096 <span class="comment">  &lt;dt&gt;\link cluster::par_kmedoids par_kmedoids\endlink&lt;/dt&gt;</span>
<a name="l00097"></a>00097 <span class="comment">  &lt;dd&gt;This class inherits from \link cluster::par_partition par_partition\endlink and it implements the CAPEK</span>
<a name="l00098"></a>00098 <span class="comment">  parallel clustering algorithm.  Functionally, CAPEK is similar to CLARA, but it is </span>
<a name="l00099"></a>00099 <span class="comment">  distributed and runs in \f$O(\frac{n}{P}log(P))\f$ time for \f$n\f$ data objects and \f$P\f$</span>
<a name="l00100"></a>00100 <span class="comment">  processes.  If \f$n = P\f$, that is, if there are only as many input data elements as processes,</span>
<a name="l00101"></a>00101 <span class="comment">  CAPEK runs in \f$O(log(P))\f$ time.  </span>
<a name="l00102"></a>00102 <span class="comment"></span>
<a name="l00103"></a>00103 <span class="comment">  The fixed-k version of CAPEK is implemented in \link cluster::par_kmedoids::capek() capek()\endlink, </span>
<a name="l00104"></a>00104 <span class="comment">  and a variant using the BIC to select a best \f$k\f$ is in \link </span>
<a name="l00105"></a>00105 <span class="comment">  cluster::par_kmedoids::xcapek() capek()\endlink.</span>
<a name="l00106"></a>00106 <span class="comment">  &lt;/dd&gt;</span>
<a name="l00107"></a>00107 <span class="comment">  &lt;/dl&gt;</span>
<a name="l00108"></a>00108 <span class="comment">  </span>
<a name="l00109"></a>00109 <span class="comment">  &lt;h3&gt;Dissimilarity Functions and Matrices&lt;/h3&gt;</span>
<a name="l00110"></a>00110 <span class="comment">  Most of the algorithms here require some sort of dissimilarity metric to run.  As with</span>
<a name="l00111"></a>00111 <span class="comment">  the STL, you can use any callable object (function or functor) as a distance function.</span>
<a name="l00112"></a>00112 <span class="comment">  See the documentation for \link cluster::par_kmedoids::xcapek() xcapek()\endlink for an </span>
<a name="l00113"></a>00113 <span class="comment">  example a dissimilarity functor.</span>
<a name="l00114"></a>00114 <span class="comment"></span>
<a name="l00115"></a>00115 <span class="comment">  The PAM algorithm, which is the basis for all the algorithms in this package, requires a </span>
<a name="l00116"></a>00116 <span class="comment">  precomputed dissimilarity matrix in order to run efficiently.  Given a set of \f$n\f$</span>
<a name="l00117"></a>00117 <span class="comment">  objects, a dissimilarity matrix is a triangular, \f$n \times n\f$ matrix \f$D\f$ where </span>
<a name="l00118"></a>00118 <span class="comment">  each element \f$D_{ij}\f$ represents the distance between the \f$i^{th}\f$ and \f$j^{th}\f$ </span>
<a name="l00119"></a>00119 <span class="comment">  objects in the data set.  It takes \f$O(n^2)\f$ time to compute a distance matrix like this.</span>
<a name="l00120"></a>00120 <span class="comment"></span>
<a name="l00121"></a>00121 <span class="comment">  We use the &lt;code&gt;boost::symmetric_matrix&lt;/code&gt; class to represent dissimilarity matrices.  This class</span>
<a name="l00122"></a>00122 <span class="comment">  provides a packed representation of an upper-triangular matrix, making it an efficient way to</span>
<a name="l00123"></a>00123 <span class="comment">  store a dissimilarity matrix for clustering.  For convenience, we have typedef&#39;d </span>
<a name="l00124"></a>00124 <span class="comment">  &lt;code&gt;boost::symmetric_matrix&lt;/code&gt; to \link cluster::dissimilarity_matrix\endlink.</span>
<a name="l00125"></a>00125 <span class="comment">  To construct a dissimilarity matrix, use \link cluster::build_dissimilarity_matrix()\endlink to do this.</span>
<a name="l00126"></a>00126 <span class="comment"></span>
<a name="l00127"></a>00127 <span class="comment">  PAM is the only algorithm</span>
<a name="l00128"></a>00128 <span class="comment">  the package that requires the use to pass in the matrix explicitly.  This is for </span>
<a name="l00129"></a>00129 <span class="comment">  efficiency reasons.  A user (or another algorithm) may want to call PAM many times</span>
<a name="l00130"></a>00130 <span class="comment">  using the same dissimilarity matrix, and with this design, the user can first build </span>
<a name="l00131"></a>00131 <span class="comment">  the matrix then call PAM without paying the (potentially very high) cost of building </span>
<a name="l00132"></a>00132 <span class="comment">  the matrix.  </span>
<a name="l00133"></a>00133 <span class="comment"></span>
<a name="l00134"></a>00134 <span class="comment"></span>
<a name="l00135"></a>00135 <span class="comment">  &lt;h2&gt;Author&lt;/h2&gt;</span>
<a name="l00136"></a>00136 <span class="comment">  Muster was implemented by Todd Gamblin at </span>
<a name="l00137"></a>00137 <span class="comment">  &lt;a href=&quot;http://www.llnl.gov&quot;&gt;Lawrence Livermore National Laboratory&lt;/a&gt;.</span>
<a name="l00138"></a>00138 <span class="comment"></span>
<a name="l00139"></a>00139 <span class="comment">  Please send questions, bug reports, or suggestions </span>
<a name="l00140"></a>00140 <span class="comment">  to &lt;a href=&quot;mailto:tgamblin@llnl.gov&quot;&gt;tgamblin@llnl.gov&lt;/a&gt;.</span>
<a name="l00141"></a>00141 <span class="comment"></span>
<a name="l00142"></a>00142 <span class="comment"></span>
<a name="l00143"></a>00143 <span class="comment">  &lt;h2&gt;References&lt;/h2&gt;</span>
<a name="l00144"></a>00144 <span class="comment">  For more details on the algorithms implemented in Muster, You can consult the following</span>
<a name="l00145"></a>00145 <span class="comment">  sources:</span>
<a name="l00146"></a>00146 <span class="comment">  </span>
<a name="l00147"></a>00147 <span class="comment">  &lt;ol&gt;</span>
<a name="l00148"></a>00148 <span class="comment">  &lt;li&gt;For more on CAPEK:</span>
<a name="l00149"></a>00149 <span class="comment">  &lt;p&gt;</span>
<a name="l00150"></a>00150 <span class="comment">  Todd Gamblin, Bronis R. de Supinski, Martin Schulz, Rob Fowler, and Daniel A. Reed.</span>
<a name="l00151"></a>00151 <span class="comment">  &lt;a href=&quot;http://www.cs.unc.edu/~tgamblin/pubs/scalable-cluster-ics10.pdf&quot;&gt;</span>
<a name="l00152"></a>00152 <span class="comment">  &lt;b&gt;Clustering Performance Data Efficiently at Massive Scales&lt;/b&gt;&lt;/a&gt;.</span>
<a name="l00153"></a>00153 <span class="comment">  &lt;i&gt;Proceedings of the International Conference on Supercomputing (ICS&#39;10)&lt;/i&gt;,</span>
<a name="l00154"></a>00154 <span class="comment">  Tsukuba, Japan, June 1-4, 2010.</span>
<a name="l00155"></a>00155 <span class="comment"></span>
<a name="l00156"></a>00156 <span class="comment">  &lt;li&gt;For more on X-Means and the Bayesian Information Criterion:</span>
<a name="l00157"></a>00157 <span class="comment">  &lt;p&gt;</span>
<a name="l00158"></a>00158 <span class="comment">  Dan Pelleg and Andrew Moore.  &lt;a href=&quot;http://www.cs.cmu.edu/~dpelleg/download/xmeans.pdf&quot;&gt;</span>
<a name="l00159"></a>00159 <span class="comment">  &lt;b&gt;X-Means: Extending K-Means with Efficient Estimation of the Number of Clusters&lt;/b&gt;&lt;/a&gt;.</span>
<a name="l00160"></a>00160 <span class="comment">  &lt;i&gt;Proceedings of the Seventeenth International Conference on Machine Learning&lt;/i&gt;, </span>
<a name="l00161"></a>00161 <span class="comment">  San Francisco, CA. June 29-July 2, 2000.  pp 727-734.</span>
<a name="l00162"></a>00162 <span class="comment"></span>
<a name="l00163"></a>00163 <span class="comment">  &lt;li&gt;For more on PAM and CLARA:</span>
<a name="l00164"></a>00164 <span class="comment">  &lt;p&gt;</span>
<a name="l00165"></a>00165 <span class="comment">  Leonard Kaufman and Peter J. Rousseeuw.  </span>
<a name="l00166"></a>00166 <span class="comment">  &lt;b&gt;&lt;a href=&quot;http://www.amazon.com/Finding-Groups-Data-Introduction-Probability/dp/0471735787&quot;&gt;Finding Groups in Data: An Introduction to Cluster Analysis&lt;/a&gt;&lt;/b&gt;. John Wiley &amp; Sons, Inc., New York. </span>
<a name="l00167"></a>00167 <span class="comment"></span>
<a name="l00168"></a>00168 <span class="comment">  &lt;/ol&gt;</span>
<a name="l00169"></a>00169 <span class="comment">*/</span>
</pre></div></div>
</div>
<hr class="footer"/>
<a href="http://www.doxygen.org/">
    <img border="0" src="doxygen.png" align="left" style="margin-right:10px;margin-left:5px;"/>
</a>
<small>
<b>Muster</b>.
Copyright &copy; 2010, <a href="http://www.llnl.gov">Lawrence Livermore National Laboratory</a>,  LLNL-CODE-433662.<br>
Distribution of Muster and its documentation is subject to terms of the Muster <a href="http://github.com/tgamblin/muster/blob/master/LICENSE">LICENSE</a>.<br>
Generated on Fri Nov 12 2010 using <a href="http://www.doxygen.org/">Doxygen 1.7.1</a>
</small>

