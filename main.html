<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<title>Muster: Main Page</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link href="doxygen.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<!-- Generated by Doxygen 1.7.1 -->
<div class="navigation" id="top">
  <div class="tabs">
    <ul class="tablist">
      <li class="current"><a href="main.html"><span>Main&nbsp;Page</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
    </ul>
  </div>
</div>
<div class="header">
  <div class="headertitle">
<h1>Muster Documentation</h1>  </div>
</div>
<div class="contents">
<div align="center">
<img src="kmedoids.png" alt="kmedoids.png"/>
</div>
<h2>Introduction</h2>
<p>The Muster library provides implementations of sequential and parallel K-Medoids clustering algorithms. It is intended as a general framework for parallel cluster analysis, particularly for performance data analysis on systems with very large numbers of processes.</p>
<p>The parallel implementations in the Muster are designed to perform well even in environments where the data to be clustered is entirely distributed. For example, many performance tools need to analyze one data element from each process in a system. To analyze this data efficiently, clustering algorithms that move as little data as possible are required. In Muster, we exploit sampled clustering algorithms to realize this efficiency.</p>
<p>The parallel algorithms in Muster are implemented using the Message Passing Interface (MPI), making them suitable for use on many of the world's largest supercomputers. They should, however, also run efficiently on your laptop.</p>
<h2>Getting Started</h2>
<h3>Partitions</h3>
<p>The algorithms in Muster are <em>partitioning algorithms</em>, that is, they divide a data set up into a set of groups, or <em>clusters</em> of data objects. Together, these groups are called a <em>clustering</em> or a <em>partitioning</em> of the data.</p>
<p>There are two classes that represent clusterings. These are as follows: </p>
<dl>
<dt><a class="el" href="structcluster_1_1partition.html">partition</a> </dt>
<dd><p class="startdd">This class represents a partitioning of a data set. It stores the clusters in the data set along with <em>representatives</em> from each of the clusters. It also stores, for every object in the data set, the cluster to which that object has been assigned.</p>
<p>A <a class="el" href="structcluster_1_1partition.html">partition</a> is entirely local to the process that owns it. It exists in one memory space and contains data about <em>all</em> objects in the data set it describes. </p>
<p class="enddd"></p>
</dd>
<dt><a class="el" href="structcluster_1_1par__partition.html">par_partition</a> </dt>
<dd>This class is similar to <a class="el" href="structcluster_1_1partition.html">partition</a>, but it is a distributed data structure. A <a class="el" href="structcluster_1_1par__partition.html">par_partition</a> object represents the results of parallel clustering algorithm. Instead of storing the cluster assignments of <em>all</em> objects in the data set, each par_partition object stores only the cluster membership for objects that are local to the calling process.  </dd>
</dl>
<p>Note that par_partition does not inherit from partition, because the classes are structurally different. One is a local, centralized data structure, and its state represents all data in the set, while the other is a distributed structure, and represents only a part of the full data set.</p>
<p>If you need to, you can convert a par_partition to a partition with the <a class="el" href="structcluster_1_1par__partition.html#a02d94b5ae788fc7ac1a397875e8d4533">par_partition::gather()</a> method, but the two classes are not interchangeable.</p>
<h3>Clustering Algorithms</h3>
<p>Classes for clustering algorithms derive from either <a class="el" href="structcluster_1_1partition.html">partition</a> or <a class="el" href="structcluster_1_1par__partition.html">par_partition</a>. The algorithms themselves are methods on these derived classes, and they store their output in the class. This allows all (or at least most of) the state for the algorithms and their output to be encapsulated in a single class.</p>
<p>Algorithms themselves are template functions on the derived classes. You can thus call them on any type of object with any number of distance metrics. Because they are template functions, you don't need to explicitly indicate the types of the things you pass to the clustering algorithms; the types are inferred from the algorithms' parameters.</p>
<p>There are two classes you should be concerned with:</p>
<dl>
<dt><a class="el" href="classcluster_1_1kmedoids.html">kmedoids</a> </dt>
<dd><p class="startdd">This class inherits from <a class="el" href="structcluster_1_1partition.html">partition</a> and contains implementations of the PAM and CLARA sequential clustering algorithms (Kaufman and Rousseeuw, 1990). PAM is an <img class="formulaInl" alt="$O(n^2)$" src="form_0.png"/>, optimal K-medoids algorithm, and CLARA is an <img class="formulaInl" alt="$O(n)$" src="form_1.png"/> implementation that executes multiple trials of PAM. These algorithms are implemented in the <a class="el" href="classcluster_1_1kmedoids.html#abb472d7a1325a47b8abde2af0509089c">pam()</a> and <a class="el" href="classcluster_1_1kmedoids.html#af4ce0293e41923a7ac472202d45f032e">clara()</a> methods.</p>
<p>PAM and CLARA both require that the caller supply <img class="formulaInl" alt="$k$" src="form_2.png"/>, the number of clusters, as a parameter to the algorithm. We have adopted a technique used by the X-Means (Pelleg and Moore, 2000) algorithm to choose an "ideal" <img class="formulaInl" alt="$k$" src="form_2.png"/> from many clustering trials using the <a class="el" href="bic_8h.html">Bayesian Information Criterion (BIC)</a>. Instead of supplying a specific <img class="formulaInl" alt="$k$" src="form_2.png"/>, the caller supplies a range of values for <img class="formulaInl" alt="$k$" src="form_2.png"/>, and the algorithms use the BIC to select the best fit from the range.</p>
<p>The BIC variants are implemented in <a class="el" href="classcluster_1_1kmedoids.html#a5116cf4e711e642a338d783029fdee3e">xpam()</a> and <a class="el" href="classcluster_1_1kmedoids.html#a309f7bb703fec73b13e2582f0ee35521">xclara()</a>. They will be slower than the fixed-k versions, as they can require many iterations of PAM or CLARA be tested to find the best <img class="formulaInl" alt="$k$" src="form_2.png"/>. </p>
<p class="enddd"></p>
</dd>
<dt><a class="el" href="classcluster_1_1par__kmedoids.html">par_kmedoids</a> </dt>
<dd><p class="startdd">This class inherits from <a class="el" href="structcluster_1_1par__partition.html">par_partition</a> and it implements the CAPEK parallel clustering algorithm. Functionally, CAPEK is similar to CLARA, but it is distributed and runs in <img class="formulaInl" alt="$O(\frac{n}{P}log(P))$" src="form_3.png"/> time for <img class="formulaInl" alt="$n$" src="form_4.png"/> data objects and <img class="formulaInl" alt="$P$" src="form_5.png"/> processes. If <img class="formulaInl" alt="$n = P$" src="form_6.png"/>, that is, if there are only as many input data elements as processes, CAPEK runs in <img class="formulaInl" alt="$O(log(P))$" src="form_7.png"/> time.</p>
<p class="enddd">The fixed-k version of CAPEK is implemented in <a class="el" href="classcluster_1_1par__kmedoids.html#a9e6466011d78034865dfcb0b15e33bf2">capek()</a>, and a variant using the BIC to select a best <img class="formulaInl" alt="$k$" src="form_2.png"/> is in <a class="el" href="classcluster_1_1par__kmedoids.html#a6d10b9b98495838ee9425932c6529855">capek()</a>.  </p>
</dd>
</dl>
<h3>Dissimilarity Functions and Matrices</h3>
<p>Most of the algorithms here require some sort of dissimilarity metric to run. As with the STL, you can use any callable object (function or functor) as a distance function. See the documentation for <a class="el" href="classcluster_1_1par__kmedoids.html#a6d10b9b98495838ee9425932c6529855">xcapek()</a> for an example a dissimilarity functor.</p>
<p>The PAM algorithm, which is the basis for all the algorithms in this package, requires a precomputed dissimilarity matrix in order to run efficiently. Given a set of <img class="formulaInl" alt="$n$" src="form_4.png"/> objects, a dissimilarity matrix is a triangular, <img class="formulaInl" alt="$n \times n$" src="form_8.png"/> matrix <img class="formulaInl" alt="$D$" src="form_9.png"/> where each element <img class="formulaInl" alt="$D_{ij}$" src="form_10.png"/> represents the distance between the <img class="formulaInl" alt="$i^{th}$" src="form_11.png"/> and <img class="formulaInl" alt="$j^{th}$" src="form_12.png"/> objects in the data set. It takes <img class="formulaInl" alt="$O(n^2)$" src="form_0.png"/> time to compute a distance matrix like this.</p>
<p>We use the <code>boost::symmetric_matrix</code> class to represent dissimilarity matrices. This class provides a packed representation of an upper-triangular matrix, making it an efficient way to store a dissimilarity matrix for clustering. For convenience, we have typedef'd <code>boost::symmetric_matrix</code> to <a class="el" href="namespacecluster.html#a3da8b7c34d3e5089d88d0a8760a4d57c">cluster::dissimilarity_matrix</a>. To construct a dissimilarity matrix, use <a class="el" href="namespacecluster.html#ac42a89b1657078539345f7ff233803cd">cluster::build_dissimilarity_matrix()</a> to do this.</p>
<p>PAM is the only algorithm the package that requires the use to pass in the matrix explicitly. This is for efficiency reasons. A user (or another algorithm) may want to call PAM many times using the same dissimilarity matrix, and with this design, the user can first build the matrix then call PAM without paying the (potentially very high) cost of building the matrix.</p>
<h2>Author</h2>
<p>Muster was implemented by Todd Gamblin at <a href="http://www.llnl.gov">Lawrence Livermore National Laboratory</a>.</p>
<p>Please send questions, bug reports, or suggestions to <a href="mailto:tgamblin@llnl.gov">tgamblin@llnl.gov</a>.</p>
<h2>References</h2>
<p>For more details on the algorithms implemented in Muster, You can consult the following sources:</p>
<ol>
<li>
<p class="startli">For more on CAPEK: </p>
<p>Todd Gamblin, Bronis R. de Supinski, Martin Schulz, Rob Fowler, and Daniel A. Reed. <a href="http://www.cs.unc.edu/~tgamblin/pubs/scalable-cluster-ics10.pdf"><b>Clustering Performance Data Efficiently at Massive Scales</b></a>. <em>Proceedings of the International Conference on Supercomputing (ICS'10)</em>, Tsukuba, Japan, June 1-4, 2010.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli">For more on X-Means and the Bayesian Information Criterion: </p>
<p>Dan Pelleg and Andrew Moore. <a href="http://www.cs.cmu.edu/~dpelleg/download/xmeans.pdf"><b>X-Means: Extending K-Means with Efficient Estimation of the Number of Clusters</b></a>. <em>Proceedings of the Seventeenth International Conference on Machine Learning</em>, San Francisco, CA. June 29-July 2, 2000. pp 727-734.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli">For more on PAM and CLARA: </p>
<p>Leonard Kaufman and Peter J. Rousseeuw. <b><a href="http://www.amazon.com/Finding-Groups-Data-Introduction-Probability/dp/0471735787">Finding Groups in Data: An Introduction to Cluster Analysis</a></b>. John Wiley &amp; Sons, Inc., New York.</p>
<p class="endli"></p>
</li>
</ol>
</div>
<hr class="footer"/>
<a href="http://www.doxygen.org/">
    <img border="0" src="doxygen.png" align="left" style="margin-right:10px;margin-left:5px;"/>
</a>
<small>
<b>Muster</b>.
Copyright &copy; 2010, <a href="http://www.llnl.gov">Lawrence Livermore National Laboratory</a>,  LLNL-CODE-433662.<br>
Distribution of Muster and its documentation is subject to terms of the Muster <a href="http://github.com/tgamblin/muster/blob/master/LICENSE">LICENSE</a>.<br>
Generated on Fri Oct 29 2010 using <a href="http://www.doxygen.org/">Doxygen 1.7.1</a>
</small>

